{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.datasets import cifar10\n",
    "import autokeras as ak\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "from autokeras.utils import pickle_from_file\n",
    "from keras.models import load_model\n",
    "from thop import profile    #pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMain():\n",
    "    # load the training and testing data, then scale it into the\n",
    "    # range [0, 1]\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"[INFO] loading CIFAR-10 data...\")\n",
    "    ((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "    trainX = trainX.astype(\"float\") / 255.0\n",
    "    testX = testX.astype(\"float\") / 255.0\n",
    "\n",
    "    # initialize the label names for the CIFAR-10 dataset\n",
    "    labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "        \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "    # initialize the output directory\n",
    "    OUTPUT_PATH = \"output\"\n",
    "\n",
    "    seconds= 12*60*60 #12 hour\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"[INFO] training model for {} seconds max...\".format(seconds))\n",
    "    model = ak.ImageClassifier(path=os.path.join(os.getcwd(),\"automodels\"),verbose=True,\n",
    "                               searcher_args={'trainer_args':{'max_iter_num':10}})\n",
    "    model.fit(trainX, trainY, time_limit=seconds)\n",
    "    print(datetime.datetime.now())\n",
    "    #saveModel(model)\n",
    "    # print(\"[INFO] final_fit\")\n",
    "    # model.final_fit(trainX, trainY, testX, testY, retrain=True)\n",
    "\n",
    "    # evaluate the Auto-Keras model\n",
    "    # score = model.evaluate(testX, testY)\n",
    "    # predictions = model.predict(testX)\n",
    "    # report = classification_report(testY, predictions,target_names=labelNames)\n",
    " \n",
    "# write the report to disk\n",
    "# p = os.path.join(OUTPUT_PATH, \"{}.txt\".format(seconds))\n",
    "# f = open(p, \"w\")\n",
    "# f.write(report)\n",
    "# f.write(\"\\nscore: {}\".format(score))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops(model):\n",
    "    return profile(model, input_size=(1, 3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModule():\n",
    "    path = os.path.join('automodels', 'module')\n",
    "    cnn_module = pickle_from_file(path)\n",
    "    return cnn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelData(module):\n",
    "    with open(\"results.txt\", \"a\") as resfile:\n",
    "        resfile.write(\"model Id,total params,file size,flops,params from profile,metric\\n\")\n",
    "    for item in module.searcher.history:  \n",
    "        model_id = item['model_id']\n",
    "        graph = module.searcher.load_model_by_id(model_id)\n",
    "        model = graph.produce_model()\n",
    "        print(model)\n",
    "        flops, params = get_flops(model)\n",
    "        file_path=os.path.join('automodels', \"{}.pt\".format(model_id))\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(\"\\nmodel Id {}:\\n   total params = {}\\n   file size = {} kB\\n   flops: {}\\n   params from profile: {}\".format(model_id,pytorch_total_params, file_size/1000, flops, params))\n",
    "        with open(\"results.txt\", \"a\") as resfile:\n",
    "            resfile.write(\"{},{},{},{},{},{}\\n\".format(model_id,pytorch_total_params, file_size/1000, flops, params,item['metric_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-01 12:01:32.587466\n",
      "[INFO] loading CIFAR-10 data...\n",
      "2019-07-01 12:01:34.132981\n",
      "[INFO] training model for 43200 seconds max...\n",
      "Saving Directory: /home/Tal/AutoDLProject/AutoDLProject/automodels\n",
      "Preprocessing the images.\n",
      "Preprocessing finished.\n",
      "\n",
      "Initializing search.\n",
      "\n",
      "local code.\n",
      "Initialization finished.\n",
      "\n",
      "\n",
      "+----------------------------------------------+\n",
      "|               Training model 0               |\n",
      "+----------------------------------------------+\n",
      "Epoch-5, Current Metric - 0.262:  36%|████████▋               | 140/387 [00:14<00:26,  9.36 batch/s]"
     ]
    }
   ],
   "source": [
    "runMain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelData(getModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
